{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Práctica 5: Pruebas estadísticas**\n",
    "\n",
    "De la práctica 3, quedaron pendientes varias hipótesis tipo \"variable tal causa diferencia entre los resultados\" que habrá que examinar con una prueba estadística adecuada. \n",
    "\n",
    "Ya que muchas pruebas estadísticas dependen de la distribución de los datos, es mejor primero dedicar un rato en examinar cuáles todas variables son normalmente distribuidas, cuáles no, y si algunas son multimodales.\n",
    "\n",
    "Cuando yo estaba en la secundaria, aún seguía de moda la idea __[calificar con una curva](https://en.wikipedia.org/wiki/Grading_on_a_curve)__ seguía muy prevalente, el razonamiento siendo esencialmente que las capacidades humanas son normalmente distribuidos y deberías tener unas pocas calificaciones altas y otras pocas bajas.\n",
    "\n",
    "En mi experiencia docente, las capacidades de los estudiantes son normales pero bimodales: el grupo de los que esfuerzan es una población cuya media y varianza difieren de las de la población que _no_ se esfuerza. De hecho, comencemos viendo cómo es la cosa con las calificaciones finales que ya conocemos para la primera y la segunda oportunidad.\n",
    "\n",
    "Tenemos fuerte razón de suponer que dependen del profe, ya que hemos visto que Moisés pone 70 a todos los que pasan segundas y que sus alumnos, mayormente de materiales, en una unidad de aprendizaje distinto, suelen sacar mayores calificaciones que los de programas educativos diversos que llevan matemáticas discretas con Elisa. Por ende, mejor analicemos aparte las calificaciones de Elisa y Moisés, separando las de primera oportunidad de las de segunda oportunidad, sabiendo a priori que esos dos factores tienen un efecto notable en las variables de interés.\n",
    "\n",
    "La examinación de normalidad es una de las tareas más básicas en la estadística aplicada, por lo cual existen diversos tutoriales en línea; conociendo mi alumnado, opto con __[un tutorial muy breve y muy claro](https://machinelearningmastery.com/a-gentle-introduction-to-normality-tests-in-python/)__ para comenzar.\n",
    "\n",
    "Lo mero primero que ocupamos es ver __[histogramas](https://plot.ly/python/histograms/)__ para darnos una idea de si es ni remotamente normal y si se ve bimodal. Para saber cómo se verían esos casos, generaré datos normalmente distribuidos, unimodales y bimodales, pseudoaleatoriamente con NumPy para comparar visualmente.\n",
    "\n",
    "Las histogramas mejor normalizarlas que tengan frecuencias relativas. Agreguemos además una línea vertical para marcar 70 que es el umbral de aprobación en licenciatura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~satuelisa/32.embed\" height=\"525px\" width=\"100%\"></iframe>\n",
      "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~satuelisa/34.embed\" height=\"525px\" width=\"100%\"></iframe>\n",
      "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~satuelisa/36.embed\" height=\"525px\" width=\"100%\"></iframe>\n",
      "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~satuelisa/38.embed\" height=\"525px\" width=\"100%\"></iframe>\n",
      "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~satuelisa/40.embed\" height=\"525px\" width=\"100%\"></iframe>\n",
      "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~satuelisa/42.embed\" height=\"525px\" width=\"100%\"></iframe>\n"
     ]
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import pandas as pd\n",
    "import ssl\n",
    "\n",
    "if getattr(ssl, '_create_unverified_context', None):\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "d = pd.read_csv(\"https://elisa.dyndns-web.com/teaching/comp/datasci/graficar.csv\")\n",
    "\n",
    "cfep = d.loc[d.profe == 'elisa'].CF1op # elisa primera\n",
    "cfes = d.loc[d.profe == 'elisa'].CF2op # elisa segunda\n",
    "cfmp = d.loc[d.profe == 'moi'].CF1op # moi primera\n",
    "cfms = d.loc[d.profe == 'moi'].CF2op # moi segunda\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "layout = {'xaxis': {'range': [0, 100]}, 'yaxis': {'range': [0, 1]}, \\\n",
    "    'shapes': [{'type': 'line', 'x0': 70, 'y0': 0, 'x1': 70, 'y1': 1, \\\n",
    "            'line': {'color': 'rgb(255, 0, 0)', 'width': 2}}]}\n",
    "\n",
    "print((py.iplot({'data': [go.Histogram(x = cfep, histnorm='probability')], 'layout': layout}, \\\n",
    "                filename='elisa-cf1ra')).embed_code)\n",
    "print((py.iplot({'data': [go.Histogram(x = cfes, histnorm='probability')], 'layout': layout}, \\\n",
    "                filename='elisa-cf2da')).embed_code)\n",
    "print((py.iplot({'data': [go.Histogram(x = cfmp, histnorm='probability')], 'layout': layout}, \\\n",
    "                filename='moi-cf1ra')).embed_code)\n",
    "print((py.iplot({'data': [go.Histogram(x = cfms, histnorm='probability')], 'layout': layout}, \\\n",
    "                filename='moi-cf2da')).embed_code)\n",
    "\n",
    "from numpy.random import randn\n",
    "from numpy.random import seed\n",
    "from numpy import concatenate\n",
    "\n",
    "seed(7) # ponemos semilla para sacar los mismo datos cada vez que se ejecuta\n",
    "unimodal = 15 * randn(1000) + 75 # varianza 15, media 75\n",
    "bimodal = concatenate((unimodal, 20 * randn(1000) + 45)) # lo mismo pero ahora con un segundo bulto agregado\n",
    "print((py.iplot({'data': [go.Histogram(x = unimodal, histnorm='probability')], 'layout': layout}, \\\n",
    "                filename=\"unimodal\")).embed_code)\n",
    "print((py.iplot({'data': [go.Histogram(x = bimodal, histnorm='probability')], 'layout': layout}, \\\n",
    "                filename=\"bimodal\")).embed_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuerda que con Plotly se puede hacer zoom a una zona arbitraria con facilidad, por lo cual no necesito preocuparme de ajustar el límite superior del eje vertical para máxima visibilidad.\n",
    "\n",
    "*Histograma de las calificaciones de Elisa de primera oportunidad*\n",
    "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~satuelisa/32.embed\" height=\"525px\" width=\"100%\"></iframe>\n",
    "\n",
    "*Histograma de las calificaciones de Elisa de segunda oportunidad*\n",
    "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~satuelisa/34.embed\" height=\"525px\" width=\"100%\"></iframe>\n",
    "\n",
    "*Histograma de las calificaciones de Moisés de primera oportunidad*\n",
    "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~satuelisa/36.embed\" height=\"525px\" width=\"100%\"></iframe>\n",
    "\n",
    "*Histograma de las calificaciones de Moisés de segunda oportunidad*\n",
    "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~satuelisa/38.embed\" height=\"525px\" width=\"100%\"></iframe>\n",
    "\n",
    "*Histograma de datos artificiales normales unimodales*\n",
    "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~satuelisa/40.embed\" height=\"525px\" width=\"100%\"></iframe>\n",
    "\n",
    "*Histograma de datos artificiales normales bimodales*\n",
    "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~satuelisa/42.embed\" height=\"525px\" width=\"100%\"></iframe>\n",
    "\n",
    "Nuevamente las gráficas están disponibles además en __[una página aparte](https://elisa.dyndns-web.com/teaching/comp/datasci/p5.html)__.\n",
    "\n",
    "Se nota que para ambos profes, las calificaciones de primera sí tienen un aire bimodal: hay dos \"picos\". Las segundas de Elisa parecen unimodales (los que no esforzaron suelen ser NC o NP) y las de Moisés están muy discretizados a niveles 70 y 80.\n",
    "\n",
    "Examinemos ahora cuáles son normales, suponiendo que fuesen *unimodales* las seis.\n",
    "\n",
    "Vamos a usar ahora la librería de __[matplotlib](https://matplotlib.org/)__. Como el Plotly permite solamente 25 gráficas gratuitas en su servicio sube, ahora empiezo a guardar las gráficas resultantes en archivos PNG para ponerlos a mi servidor web (hay una suscripción estudiantíl por 99 dólares al año, pero ni soy estudiante y es mucho dinero eso); si tuvieramos una suscripción, se podrían mandar las figuras de matplotlib a la nube también."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "import matplotlib as plt\n",
    "from numpy.random import randn\n",
    "from numpy import concatenate\n",
    "\n",
    "d = pd.read_csv(\"graficar.csv\")\n",
    "cfep = d.loc[d.profe == 'elisa'].CF1op # elisa primera\n",
    "cfes = d.loc[d.profe == 'elisa'].CF2op # elisa segunda\n",
    "cfmp = d.loc[d.profe == 'moi'].CF1op # moi primera\n",
    "cfms = d.loc[d.profe == 'moi'].CF2op # moi segunda\n",
    "\n",
    "layout = {'xaxis': {'range': [0, 100]}, 'yaxis': {'range': [0, 1]}, \\\n",
    "    'shapes': [{'type': 'line', 'x0': 70, 'y0': 0, 'x1': 70, 'y1': 1, \\\n",
    "            'line': {'color': 'rgb(255, 0, 0)', 'width': 2}}]}\n",
    "\n",
    "po.iplot({'data': [go.Histogram(x = cfms, histnorm='probability')], 'layout': layout}, filename='moi-cf2da')\n",
    "\n",
    "unimodal = 15 * randn(1000) + 75 # varianza 15, media 75\n",
    "bimodal = concatenate((unimodal, 20 * randn(1000) + 45)) # lo mismo pero ahora con un segundo bulto agregado\n",
    "\n",
    "f = qqplot(cfep, line='s')\n",
    "plt.save(f, \"qq_cfep.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Son las figuras __[7](https://elisa.dyndns-web.com/teaching/comp/datasci/p5.html#f7)__ a __[12](https://elisa.dyndns-web.com/teaching/comp/datasci/p5.html#12)__:\n",
    "\n",
    "![Figura 7](https://elisa.dyndns-web.com/teaching/comp/datasci/qq_cfep.png)\n",
    "\n",
    "![Figura 8](https://elisa.dyndns-web.com/teaching/comp/datasci/qq_cfes.png)\n",
    "\n",
    "![Figura 9](https://elisa.dyndns-web.com/teaching/comp/datasci/qq_cfmp.png)\n",
    "\n",
    "![Figura 10](https://elisa.dyndns-web.com/teaching/comp/datasci/qq_cfms.png)\n",
    "\n",
    "![Figura 11](https://elisa.dyndns-web.com/teaching/comp/datasci/qq_cfum.png)\n",
    "\n",
    "![Figura 12](https://elisa.dyndns-web.com/teaching/comp/datasci/qq_cfbm.png)\n",
    "\n",
    "La raya roja aparece únicamente en aquellas que son más o menos normales: \n",
    "* Las de Elisa de primera *no* parecen normales sino más bien bimodales\n",
    "* Las de Elisa de segunda *no* parecen normales sino más bien bimodales\n",
    "* Las de Moisés de primera casi hacen una recta, pero tienen dos estancamientos (probablemente a 70 y a 100)\n",
    "* Las de Moisés de segunda ya sabíamos que iban a ser raros\n",
    "* Las normales unimodales se portan perfecto como deben\n",
    "* Las normales bimodales tienen una curvatura debido a su multimodalidad\n",
    "\n",
    "Para esta práctica, vamos a ignorar el hecho que son probablemente mezclas de dos distribuciones normales; en una práctica futura usaremos un __[algoritmo para separar las dos poblaciones](https://scikit-learn.org/stable/modules/mixture.html)__ pero por hoy simplemente checamos sí o no se pueden aplicar pruebas para datos normales o si serán más bien pruebas no-paramétricas.\n",
    "\n",
    "Intentemos primero con una prueba Shapiro-Wilk para los seis conjuntos de datos.\n",
    "\n",
    "Esto sí se puede ejecutar en jupyter ya que no intenta mandar nada a Plotly a la cuenta que ya se llenó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e1ra 0.89 0.000\n",
      "no parece ser normal con nivel de significancia 0.05\n",
      "e2da 0.91 0.000\n",
      "no parece ser normal con nivel de significancia 0.05\n",
      "m1ra 0.95 0.002\n",
      "no parece ser normal con nivel de significancia 0.05\n",
      "m2da 0.42 0.000\n",
      "no parece ser normal con nivel de significancia 0.05\n",
      "unim 1.00 0.834\n",
      "aceptablemente normal con nivel de significancia 0.05\n",
      "bim 0.99 0.000\n",
      "no parece ser normal con nivel de significancia 0.05\n",
      "e1ra 0.89 0.000\n",
      "no parece ser normal con nivel de significancia 0.01\n",
      "e2da 0.91 0.000\n",
      "no parece ser normal con nivel de significancia 0.01\n",
      "m1ra 0.95 0.002\n",
      "no parece ser normal con nivel de significancia 0.01\n",
      "m2da 0.42 0.000\n",
      "no parece ser normal con nivel de significancia 0.01\n",
      "unim 1.00 0.834\n",
      "aceptablemente normal con nivel de significancia 0.01\n",
      "bim 0.99 0.000\n",
      "no parece ser normal con nivel de significancia 0.01\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from numpy.random import randn\n",
    "from numpy.random import seed\n",
    "from numpy import concatenate, isnan\n",
    "import ssl\n",
    "\n",
    "if getattr(ssl, '_create_unverified_context', None):\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "d = pd.read_csv(\"https://elisa.dyndns-web.com/teaching/comp/datasci/graficar.csv\")\n",
    "\n",
    "seed(7)\n",
    "nd = 15 * randn(1000) + 75\n",
    "datos = {'e1ra': d.loc[d.profe == 'elisa'].CF1op , \\\n",
    "         'e2da': d.loc[d.profe == 'elisa'].CF2op, \\\n",
    "         'm1ra': d.loc[d.profe == 'moi'].CF1op, \\\n",
    "         'm2da': d.loc[d.profe == 'moi'].CF2op, \\\n",
    "         'unim': nd, \\\n",
    "         'bim': concatenate((nd, 20 * randn(1000) + 45))}\n",
    "\n",
    "from scipy.stats import shapiro\n",
    "for alpha in [0.05, 0.01]:\n",
    "    for data in datos:\n",
    "        crudos = datos[data]\n",
    "        s, p = shapiro(crudos[~isnan(crudos)]) # es MUY importante quitar los NaN\n",
    "        print('{:s} {:.2f} {:.3f}'.format(data, s, p))\n",
    "        if p > alpha:\n",
    "            print('aceptablemente normal con nivel de significancia', alpha)\n",
    "        else:\n",
    "            print('no parece ser normal con nivel de significancia', alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como era de esperarse, lo único que pasa la prueba de normalidad son los datos artificiales normales. Todo el resto truena con ambos valores de significancia. \n",
    "\n",
    "Haremos __[pruebas no-paramétricas](https://machinelearningmastery.com/nonparametric-statistical-significance-tests-in-python/)__, entonces.\n",
    "\n",
    "Con una prueba de *Mann-Whitney*, probaremos si la combo profe-tema efecta en la calificación (los de Moisés de ciencia de materiales versus los de Elisa de mates discretas); esto sirve más adelante en esta práctica para los demás casos pendientes que teníamos por comprobar donde un factor de interés exactamente dos niveles (sí o no trabaja, sí o no ha ido a asesorías, sí o no sabe su promedio, etc).\n",
    "\n",
    "Con una prueba de *Wilcoxon* podemos probar si los que quedan en segundas mejoran en comparación son su primera oportunidad (se necesitan pares de datos para esta prueba) o si cambia la percepción de la calificación esperara entre dos encuestas. \n",
    "\n",
    "Con una prueba *Kruskal-Wallis* podemos probar si la hora-salón afecta (Elisa tiene tres grupos en distintos salones). Esto sirve para los demás casos pendientes que teníamos por comprobar donde un factor de interés tiene más de dos niveles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primera oportunidad\n",
      "417 95 11421.00 0.000\n",
      "se ven diferentemente distribuidos con nivel de significancia 0.05\n",
      "417 95 11421.00 0.000\n",
      "se ven diferentemente distribuidos con nivel de significancia 0.01\n",
      "Segunda oportunidad\n",
      "hay muy pocos datos como para obtener un resultado confiable\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from numpy.random import randn\n",
    "from numpy.random import seed\n",
    "from numpy import concatenate, isnan\n",
    "from scipy.stats import mannwhitneyu\n",
    "import ssl\n",
    "\n",
    "if getattr(ssl, '_create_unverified_context', None):\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "d = pd.read_csv(\"https://elisa.dyndns-web.com/teaching/comp/datasci/graficar.csv\")\n",
    "\n",
    "seed(7)\n",
    "nd = 15 * randn(1000) + 75\n",
    "datos = {'e1ra': d.loc[d.profe == 'elisa'].CF1op , \\\n",
    "         'e2da': d.loc[d.profe == 'elisa'].CF2op, \\\n",
    "         'm1ra': d.loc[d.profe == 'moi'].CF1op, \\\n",
    "         'm2da': d.loc[d.profe == 'moi'].CF2op, \\\n",
    "         'unim': nd, \\\n",
    "         'bim': concatenate((nd, 20 * randn(1000) + 45))}\n",
    "\n",
    "def prueba(c1, c2):\n",
    "    d1 = c1[~isnan(c1)]\n",
    "    d2 = c2[~isnan(c2)]\n",
    "    n1 = len(d1)\n",
    "    n2 = len(d2)\n",
    "    if min(n1, n2) < 20:\n",
    "        print('hay muy pocos datos como para obtener un resultado confiable')\n",
    "        return\n",
    "    for alpha in [0.05, 0.01]:\n",
    "        s, p = mannwhitneyu(d1, d2)\n",
    "        print('{:d} {:d} {:.2f} {:.3f}'.format(n1, n2, s, p))\n",
    "        if p > alpha:\n",
    "            print('son igualmente distribuidos con nivel de significancia', alpha)\n",
    "        else:\n",
    "            print('se ven diferentemente distribuidos con nivel de significancia', alpha)\n",
    "\n",
    "print(\"Primera oportunidad\")\n",
    "prueba(datos['e1ra'], datos['m1ra'])\n",
    "print(\"Segunda oportunidad\")\n",
    "prueba(datos['e2da'], datos['m2da'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primera oportunidad, definitivamente hay diferencia, mientras en segunda oportunidad no es posible concluir.\n",
    "\n",
    "Ahora lo de Wilcoxon para si\n",
    "* mejoran en segundas (los que presentaron ambas oportunidades)\n",
    "* estiman diferente en la primera y la segunda encuesta (los que contestaron ambas encuestas)\n",
    "* estiman diferente en la segunda y la tercera encuesta (los que contestaron ambas encuestas)\n",
    "* estiman diferente en la primera y la tercera encuesta (los que contestaron ambas encuestas)\n",
    "\n",
    "Habrá que cuantificar los estimados a un sólo número cada categoría para la prueba; vamos con\n",
    "* tercera: 20\n",
    "* segunda: 50 \n",
    "* 70-79: 75\n",
    "* 80-89: 85\n",
    "* 90-100: 95\n",
    "para los tres casos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segundas 70 70 0.00 0.000\n",
      "se ven diferentemente distribuidos con nivel de significancia 0.05\n",
      "segundas 70 70 0.00 0.000\n",
      "se ven diferentemente distribuidos con nivel de significancia 0.01\n",
      "ini_vs_mcu 309 309 592.00 0.000\n",
      "se ven diferentemente distribuidos con nivel de significancia 0.05\n",
      "ini_vs_mcu 309 309 592.00 0.000\n",
      "se ven diferentemente distribuidos con nivel de significancia 0.01\n",
      "mcu_vs_ord 236 236 2129.50 0.041\n",
      "se ven diferentemente distribuidos con nivel de significancia 0.05\n",
      "mcu_vs_ord 236 236 2129.50 0.041\n",
      "son igualmente distribuidos con nivel de significancia 0.01\n",
      "ini_vs_orc 241 241 290.00 0.000\n",
      "se ven diferentemente distribuidos con nivel de significancia 0.05\n",
      "ini_vs_orc 241 241 290.00 0.000\n",
      "se ven diferentemente distribuidos con nivel de significancia 0.01\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from numpy import isnan, nan\n",
    "import ssl\n",
    "\n",
    "if getattr(ssl, '_create_unverified_context', None):\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "d = pd.read_csv(\"https://elisa.dyndns-web.com/teaching/comp/datasci/graficar.csv\")\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "pares = dict()\n",
    "pedazo = d.loc[~isnan(d.CF1op) & ~isnan(d.CF2op)]\n",
    "pares['segundas'] = (pedazo.CF1op, pedazo.CF2op)\n",
    "pedazo = d.loc[~isnan(d.eI) & ~isnan(d.eM)]\n",
    "pares['ini_vs_mcu'] = (pedazo.eI, pedazo.eM)\n",
    "pedazo = d.loc[~isnan(d.eM) & ~isnan(d.eF)]\n",
    "pares['mcu_vs_ord'] = (pedazo.eM, pedazo.eF)\n",
    "pedazo = d.loc[~isnan(d.eI) & ~isnan(d.eF)]\n",
    "pares['ini_vs_ord'] = (pedazo.eI, pedazo.eF)\n",
    "\n",
    "for datos in pares:\n",
    "    (d1, d2) = pares[datos]\n",
    "    n1 = len(d1)\n",
    "    n2 = len(d2)\n",
    "    if min(n1, n2) < 20:\n",
    "        print('hay muy pocos datos de {:s} como para obtener un resultado confiable'.format(datos))\n",
    "    else:\n",
    "        for alpha in [0.05, 0.01]:\n",
    "            s, p = wilcoxon(d1, d2)\n",
    "            print('{:s} {:d} {:d} {:.2f} {:.3f}'.format(datos, n1, n2, s, p))\n",
    "            if p > alpha:\n",
    "                print('son igualmente distribuidos con nivel de significancia', alpha)\n",
    "            else:\n",
    "                print('se ven diferentemente distribuidos con nivel de significancia', alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El único caso donde *no* se ve diferencia es entre la encuesta de medio curso y al encuesta al final del semestre sobre la calificación esperada, salvo que si la significancia es de 0.01, pero con datos tan poco confiables como encuestas y con solamente 200 y pico datos, es mejor quedarse con 0.05 de todas formas. Entonces sí hay diferencia estadísticamente significativa: los alumnos cambian de opinión sobre su calificación a lo largo del semestre y el desempeño sí es diferente en primera y en segunda oportunidad...  \n",
    "\n",
    "Con esta prueba podemos examinar si hay diferencias en el desempeño (medido con la calificación obtenida) en primera o segunda oportunidad entre\n",
    "* los que entraron a la facu en enero y los que entraron en agostot\n",
    "* los que trabajan en función de cuántas horas trabajan\n",
    "* los que saben cuántos créditos valen las unidades de aprendizaje y los que no saben\n",
    "* los que saben cuántas horas es un crédito y los que no saben \n",
    "* los que saben ambos versus los que no saben algo\n",
    "* los que no saben ninguno versus los que saben algo\n",
    "* los que fueron a asesorías y los que no fueron\n",
    "* los que dicen que estudian más desempeñan diferentemente (en las tres encuestas)\n",
    "aún cuando los datos *no* forman pares. Es decir, ya no necesitan tener el mismo número de observaciones los conjuntos de datos que se usa como entrada a la prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sem de ingreso 356 156 22.25 0.000\n",
      "hay por lo menos un factor que causa diferencia con nivel de significancia 0.05\n",
      "sem de ingreso 356 156 22.25 0.000\n",
      "hay por lo menos un factor que causa diferencia con nivel de significancia 0.01\n",
      "hrs de trabajo 52 31 31 48 53 11.75 0.019\n",
      "hay por lo menos un factor que causa diferencia con nivel de significancia 0.05\n",
      "hrs de trabajo 52 31 31 48 53 11.75 0.019\n",
      "el factor no parece causar diferencia con nivel de significancia 0.01\n",
      "sabe cr 447 65 9.16 0.002\n",
      "hay por lo menos un factor que causa diferencia con nivel de significancia 0.05\n",
      "sabe cr 447 65 9.16 0.002\n",
      "hay por lo menos un factor que causa diferencia con nivel de significancia 0.01\n",
      "sabe hrs 491 21 8.22 0.004\n",
      "hay por lo menos un factor que causa diferencia con nivel de significancia 0.05\n",
      "sabe hrs 491 21 8.22 0.004\n",
      "hay por lo menos un factor que causa diferencia con nivel de significancia 0.01\n",
      "sabe ambos 501 11 4.89 0.027\n",
      "hay por lo menos un factor que causa diferencia con nivel de significancia 0.05\n",
      "sabe ambos 501 11 4.89 0.027\n",
      "el factor no parece causar diferencia con nivel de significancia 0.01\n",
      "sabe prom 187 325 52.81 0.000\n",
      "hay por lo menos un factor que causa diferencia con nivel de significancia 0.05\n",
      "sabe prom 187 325 52.81 0.000\n",
      "hay por lo menos un factor que causa diferencia con nivel de significancia 0.01\n",
      "fue a asesorias 184 20 1.72 0.190\n",
      "el factor no parece causar diferencia con nivel de significancia 0.05\n",
      "fue a asesorias 184 20 1.72 0.190\n",
      "el factor no parece causar diferencia con nivel de significancia 0.01\n",
      "estudio ini 127 70 131 7 61 6.04 0.196\n",
      "el factor no parece causar diferencia con nivel de significancia 0.05\n",
      "estudio ini 127 70 131 7 61 6.04 0.196\n",
      "el factor no parece causar diferencia con nivel de significancia 0.01\n",
      "hay muy pocos datos de estudio mcu como para obtener un resultado confiable\n",
      "hay muy pocos datos de estudio ord como para obtener un resultado confiable\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from numpy import isnan, nan\n",
    "import ssl\n",
    "\n",
    "if getattr(ssl, '_create_unverified_context', None):\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "d = pd.read_csv(\"https://elisa.dyndns-web.com/teaching/comp/datasci/graficar.csv\")\n",
    "\n",
    "from scipy.stats import kruskal\n",
    "\n",
    "niveles = dict()\n",
    "\n",
    "niveles['sem de ingreso'] = [pedazo[1].CF1op.dropna() for pedazo in d.groupby(['inicio'])]\n",
    "niveles['hrs de trabajo'] = [pedazo[1].CF1op.dropna() for pedazo in d.groupby(['hrsNum'])]\n",
    "niveles['sabe cr'] = [pedazo[1].CF1op.dropna() for pedazo in d.groupby(['sabeCreditos'])]\n",
    "niveles['sabe hrs'] = [pedazo[1].CF1op.dropna() for pedazo in d.groupby(['sabeHoras'])]\n",
    "niveles['sabe ambos'] = [pedazo[1].CF1op.dropna() for pedazo in d.groupby(['sabeAmbos'])]\n",
    "niveles['sabe prom'] = [pedazo[1].CF1op.dropna() for pedazo in d.groupby(['sabePromedio'])]\n",
    "niveles['fue a asesorias'] = [pedazo[1].CF1op.dropna() for pedazo in d.groupby(['fueAses'])]\n",
    "niveles['estudio ini'] = [pedazo[1].CF1op.dropna() for pedazo in d.groupby(['hrsEstudio_ini'])]\n",
    "niveles['estudio mcu'] = [pedazo[1].CF1op.dropna() for pedazo in d.groupby(['hrsEstudio_mcu'])]\n",
    "niveles['estudio ord'] = [pedazo[1].CF1op.dropna() for pedazo in d.groupby(['hrsEstudio'])]\n",
    "\n",
    "for datos in niveles:\n",
    "    niv = niveles[datos]\n",
    "    if min([len(n) for n in niv]) < 5:\n",
    "        print('hay muy pocos datos de {:s} como para obtener un resultado confiable'.format(datos))\n",
    "    else:\n",
    "        for alpha in [0.05, 0.01]:\n",
    "            s, p = kruskal(*niv)\n",
    "            print('{:s} {:s} {:.2f} {:.3f}'.format(datos, ' '.join([str(len(n)) for n in niv]), s, p))\n",
    "            if p > alpha:\n",
    "                print('el factor no parece causar diferencia con nivel de significancia', alpha)\n",
    "            else:\n",
    "                print('hay por lo menos un factor que causa diferencia con nivel de significancia', alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horas de trabajo no afectan en la calificación de primera oportunidad de una manera estadísticamente significativa con un nivel de significancia de 0.01, pero sí con 0.05.\n",
    "\n",
    "Lo mismo pasa con el sí o no saben cuántos créditos es la unidad de aprendizaje y también cuántas horas de estudio es un crédito.\n",
    "\n",
    "Asesorías no tienen efecto estadísticamente significativo con ninguno de los valores de alpha.\n",
    "\n",
    "Tampoco lo tienen las horas planeadas de estudio que planean o dicen que realizaron los alumnos, mientras con las otras dos encuestas ya son tan pocas personas que seleccionaron algunas de las respuestas que no se puede analizar (habría que combinar las que tienen pocas respuestas con las más cercanas, pero ya da hueva a estas alturas, por lo cual es mejor revisitar el tema en el futuro).\n",
    "\n",
    "![Basta](https://cdn-images-1.medium.com/max/1600/0*lu4eTYjfc-Gs7U9b.png)\n",
    "\n",
    "En su quinto reporte, apliquen por lo menos tres diferentes tipos de pruebas estadísticas de hipótesis, argumentando por qué las pruebas con aplicables en cada caso. Pueden ser las mismas de los ejemplos o algunas otras (hay docenas si no cientos disponibles)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
